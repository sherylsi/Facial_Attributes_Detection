{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0nEcnN4MweC"
   },
   "source": [
    "# Facial Attributes Classifier Models\n",
    "\n",
    "In this file I classify images of specific facial att. and will use **transfer learning** to improve results.\n",
    "\n",
    "**Note:** It is recommended to run this file on **Google Colab** with GPU acceleration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contain functions for: \n",
    "- Data Preparation\n",
    "    * preprocess image\n",
    "    * split train test for\n",
    "        * Positive Class - 1\n",
    "        * Negative Class - 0\n",
    "    * Create Labeled DF for train and test (file_id, label, image)\n",
    "    \n",
    "- Apply ImageDataGenerator class\n",
    "    * Train Set\n",
    "    * Validation Set\n",
    "    * Test Set\n",
    "    \n",
    "- Modeling using DeepFace assemble or standalone of: \n",
    "    \n",
    "    * 'vgg_face': VGGFace,\n",
    "    * 'open_face': OpenFace,\n",
    "    * 'facenet': Facenet,\n",
    "    * 'deep_face': FbDeepFace,\n",
    "    * 'deep_id': DeepID,\n",
    "    * \"emotion\": Emotion,\n",
    "    * \"age\": Age,\n",
    "    * \"gender\": Gender,\n",
    "    * \"race\": Race\n",
    "      \n",
    "    - Transfer Learning with:\n",
    "        * Callback\n",
    "        * Optimzing\n",
    "        * Fitting\n",
    "        * Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "veZjl0m4U3PQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from varname import nameof\n",
    "import shutil \n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.set_printoptions(precision=2)\n",
    "# pd.options.display.max_seq_items = 20\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 15),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Input, Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import optimizers\n",
    "import tensorboard\n",
    "from sklearn.metrics import classification_report\n",
    "#facial analysis\n",
    "from deepface.extendedmodels import Age, Gender, Race, Emotion\n",
    "from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace, DeepID\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 1093,
     "status": "ok",
     "timestamp": 1599833164676,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "YXKuO394Rm_O",
    "outputId": "22b37248-b17f-4a5b-aaa3-193bde217366"
   },
   "source": [
    "### Constant path for Colab or Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imagepath(file):\n",
    "    \"\"\"\n",
    "    function look for image files in the relevant directory determine by condition\n",
    "    param: file - str name of image file\n",
    "    return: relevent file path\n",
    "    \"\"\"\n",
    "    temp_file = int(file.split('.')[0][-6:])\n",
    "    if temp_file <= 70000:\n",
    "        IMAGEPATH = '/Users/tal/Google Drive/Cellebrite/Datasets/face_att/1/'\n",
    "    elif temp_file > 70000 and temp_file < 140000:\n",
    "        IMAGEPATH = '/Users/tal/Google Drive/Cellebrite/Datasets/face_att/2/'\n",
    "    elif temp_file >= 140000:\n",
    "        IMAGEPATH = '/Users/tal/Google Drive/Cellebrite/Datasets/face_att/3/'\n",
    "    else:\n",
    "        IMAGEPATH = '/Users/tal/Google Drive/Cellebrite/Datasets/face_att/'\n",
    "    return os.path.join(IMAGEPATH, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "L3e7h8C_UkXD"
   },
   "outputs": [],
   "source": [
    "if os.getcwd() != '/content':\n",
    "    IMAGEPATH = '/Users/tal/Google Drive/Cellebrite/Datasets/face_att/'\n",
    "    IND_FILE = '/Users/tal/Google Drive/Cellebrite/files list.csv'\n",
    "    try:\n",
    "        FACEPATH = os.path.join(IMAGEPATH,'face_att')\n",
    "    except:\n",
    "        FACEPATH = find_imagepath(file)\n",
    "else:\n",
    "    drive.mount('/content/drive')\n",
    "    IND_FILE = '/content/drive/My Drive/Cellebrite/files list.csv'\n",
    "    IMAGEPATH = '/content/drive/My Drive/Cellebrite/Datasets'\n",
    "    FACEPATH = os.path.join(IMAGEPATH,'face_att')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5MeVhSAOYW2"
   },
   "source": [
    "# Data Preparation\n",
    "Data splited to sets for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(data, img_size):\n",
    "    data_img = []\n",
    "    IMG_WIDTH = img_size\n",
    "    IMG_HEIGHT = img_size\n",
    "    for i in data.iloc[:, 0]:\n",
    "        image = cv2.imread(find_imagepath(i))\n",
    "        image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.cv2.CAP_OPENNI_GRAY_IMAGE)\n",
    "        image = np.array(image).astype('float32') / 255.\n",
    "        data_img.append(image)\n",
    "    return data_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(IND_FILE)\n",
    "cols = df.columns.tolist()\n",
    "accessories_label = [l for l in cols if l.startswith(\"Wearing\")]\n",
    "hair_label = [l for l in cols if \"Hair\" in l and not l.startswith('0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Black_Hair',\n",
       "  'Blond_Hair',\n",
       "  'Brown_Hair',\n",
       "  'Gray_Hair',\n",
       "  'Receding_Hairline',\n",
       "  'Straight_Hair',\n",
       "  'Wavy_Hair'],\n",
       " ['Wearing_Earrings',\n",
       "  'Wearing_Hat',\n",
       "  'Wearing_Lipstick',\n",
       "  'Wearing_Necklace',\n",
       "  'Wearing_Necktie'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hair_label, accessories_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def data_preprocess(index_file, labels, balance, binary, img_size=224):\n",
    "        \"\"\"\n",
    "        function read \"files list.csv\" and return train test filename and pixel represtation for the called label\n",
    "        params: index_file - CSV for indexed labels\n",
    "                labels - list of labels from csv columns name\n",
    "                balance - int for a specific balanced set size\n",
    "                binary - bool if user want also the negative class (= 0_  +  labels)\n",
    "        returns: balanced train test set for positive and negative or multiclass labels\n",
    "        \"\"\"\n",
    "        class_label = []\n",
    "        # read labels file list\n",
    "        if isinstance(labels, str):\n",
    "            df = pd.read_csv(index_file, usecols=[labels])\n",
    "        else:\n",
    "            df = pd.read_csv(index_file, usecols=labels)\n",
    "            l = len(labels)\n",
    "            class_label = {k:v for k,v in zip(range(1, l+1), labels)}\n",
    "            print(label_class)\n",
    "        df_label = df[(df[labels] != 0) & (df[labels] != '0')]\n",
    "        if len(df_label) < balance:\n",
    "            print(f'The number of sample ({balance}) you asked for the label {labels} is higher than the number of '\n",
    "                  f'sample available {len(df_label)}\\nProcess Continue with {len(df_label)} Images')\n",
    "            balance = len(df_label)\n",
    "\n",
    "        # Get the label folder if not provide or label files are mix in the same folder\n",
    "        #     folder = df_label[labels].apply(lambda x: '_'.join(str(x).split('_')[:-1])).unique()\n",
    "        #     folder = [f for f in folder if f]\n",
    "\n",
    "        # Train Test Split\n",
    "        if balance is None:\n",
    "            train_size = int(len(df_label) * 0.8)\n",
    "        else:\n",
    "            train_size = int(balance * 0.8)  # int(input('Please enter 2nd class train size: '))\n",
    "\n",
    "        train = df_label[:train_size]\n",
    "        test = df_label[train_size:balance]\n",
    "\n",
    "        print(f\"Starting Image Preprocessing\")\n",
    "        # Preprocess train image\n",
    "        if binary:\n",
    "            class_label = [np.zeros(len(train)) if labels[0].isdigit() else np.ones(len(train))]            \n",
    "        train_img = img_preprocess(train, img_size)\n",
    "        train = pd.DataFrame({'files': train.iloc[:, 0], 'label': np.array(*class_label).astype(str), 'image': pd.Series(train_img)})\n",
    "\n",
    "        # Preprocess test image\n",
    "        if binary or run_neg:\n",
    "            class_label = [np.zeros(len(test)) if labels[0].isdigit() else np.ones(len(test))]\n",
    "        \n",
    "        test_img = img_preprocess(test, img_size)\n",
    "        test = pd.DataFrame({'files': test.iloc[:, 0], 'label': np.array(*class_label).astype(str), 'image': test_img})\n",
    "        print('Done!')\n",
    "\n",
    "        if binary:\n",
    "            print('Creating Negative Class')\n",
    "#             run_neg = True\n",
    "            \n",
    "            # check for balanced data\n",
    "            try:\n",
    "                assert pd.read_csv(index_file, usecols=['0_' + labels]).shape[0] <= balance\n",
    "            except AssertionError:\n",
    "                print(f\"Negative class files:\\t {pd.read_csv(index_file, usecols=['0_' + labels]).shape[0]}\")\n",
    "\n",
    "            # Add Negative class\n",
    "            train_n, test_n = data_preprocess(index_file, '0_' + labels, balance, binary=False)\n",
    "            train = pd.concat([train, train_n], axis=0)\n",
    "            test = pd.concat([test, test_n], axis=0)\n",
    "            print('Shape with Negative class:')\n",
    "        print(f'Train shape: \\t{np.array(train).shape}\\nTest shape: \\t{np.array(test).shape}')\n",
    "\n",
    "        # Verification\n",
    "        try:\n",
    "            assert test['files'].nunique() == len(test)\n",
    "            assert train['files'].nunique() == len(train)\n",
    "            print(\"Assertions Passed! Sets are image files W/O duplication\")\n",
    "        except AssertionError:\n",
    "            print(\"Assertions Failed\")\n",
    "\n",
    "#         train = shuffle(train)\n",
    "#         test = shuffle(test)\n",
    "\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Image Preprocessing\n",
      "Done!\n",
      "Creating Negative Class\n",
      "Negative class files:\t 198052\n",
      "Starting Image Preprocessing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2f542aceb35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# label = ['Wearing']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIND_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eyeglasses'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-af7d2188fe1c>\u001b[0m in \u001b[0;36mdata_preprocess\u001b[0;34m(index_file, labels, balance, binary, img_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Add Negative class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtrain_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-af7d2188fe1c>\u001b[0m in \u001b[0;36mdata_preprocess\u001b[0;34m(index_file, labels, balance, binary, img_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mclass_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'files'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Preprocess test image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "# label = ['Wearing']\n",
    "train, test = data_preprocess(IND_FILE, 'Eyeglasses', 5000, True, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [test['files'][test['label']=='1.0'], test['files'][test['label']=='0.0']]\n",
    "titles = ['Eyeglasses', 'W/O Eyeglasses']\n",
    "\n",
    "for l, j, t in zip(labels, range(1,3), titles):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(5):\n",
    "        file = l.sample().values[0]\n",
    "        file_path = find_imagepath(file)\n",
    "        img = mpimg.imread(file_path)\n",
    "        ax = plt.subplot(2, 5, i+1)\n",
    "        plt.suptitle(t)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhWSv75qOYW5"
   },
   "source": [
    "#### `ImageDataGenerator` class \n",
    "load our dataset as an iterator (not keeping it all in memory at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3e7h8C_UkXD"
   },
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen = ImageDataGenerator(validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 38556,
     "status": "ok",
     "timestamp": 1599833202206,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "o2PXUMLbXg9H",
    "outputId": "e7bb74d4-2e9d-404a-85a4-971f37c8170c"
   },
   "outputs": [],
   "source": [
    "train_data = img_gen.flow_from_dataframe(train,\n",
    "                                         directory=FACEPATH+'/1/',\n",
    "                                         x_col='files', \n",
    "                                         y_col='label', \n",
    "                                         class_mode='binary', \n",
    "                                         batch_size=64, \n",
    "                                         target_size=(224, 224), \n",
    "                                         subset='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 38556,
     "status": "ok",
     "timestamp": 1599833202206,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "o2PXUMLbXg9H",
    "outputId": "e7bb74d4-2e9d-404a-85a4-971f37c8170c"
   },
   "outputs": [],
   "source": [
    "valid_data = img_gen.flow_from_dataframe(train,\n",
    "                                         directory=FACEPATH+'/1/',\n",
    "                                         x_col='files', \n",
    "                                         y_col='label', \n",
    "                                         class_mode='binary', \n",
    "                                         batch_size=64, \n",
    "                                         target_size=(224, 224), \n",
    "                                         subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen_test = ImageDataGenerator()\n",
    "test_data = img_gen_test.flow_from_dataframe(test,\n",
    "                                        directory=FACEPATH+'/1/',\n",
    "                                        x_col='files', \n",
    "                                        y_col='label',\n",
    "                                        class_mode=None, \n",
    "                                        target_size=(224,224), \n",
    "                                        batch_size=64, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FaceDetection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model):\n",
    "    \"\"\"\n",
    "    load face detection models\n",
    "    \"\"\"\n",
    "    models = {'vgg_face': VGGFace,\n",
    "              'open_face': OpenFace,\n",
    "              'facenet': Facenet,\n",
    "              'deep_face': FbDeepFace,\n",
    "              'deep_id': DeepID,\n",
    "              \n",
    "              \"emotion\": Emotion,\n",
    "              \"age\": Age,\n",
    "              \"gender\": Gender,\n",
    "              \"race\": Race}\n",
    "    \n",
    "    return models[model].loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 47200,
     "status": "ok",
     "timestamp": 1599833210873,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "ej0sUvnZzdag",
    "outputId": "2939095f-9f76-45f4-8b1a-9a314b70026a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_summary(model):\n",
    "    print(f\"Input_shape:\\t{model.input_shape}\\nOutput_shape:\\t{model.output_shape}\\nParams:\\t{model.count_params()} \\\n",
    "    \\nLayers:\\t{len(model.layers)}\\n\\n\")\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 47200,
     "status": "ok",
     "timestamp": 1599833210873,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "ej0sUvnZzdag",
    "outputId": "2939095f-9f76-45f4-8b1a-9a314b70026a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# By default, it loads weights pre-trained on ImageNet. \n",
    "vgg16 = tf.keras.applications.vgg16.VGG16(include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 47200,
     "status": "ok",
     "timestamp": 1599833210873,
     "user": {
      "displayName": "Tal T",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2JEqenXMt8KQ0yJDTMq6jL2fP9wTV8ULQdEoZaw=s64",
      "userId": "04492565207152809606"
     },
     "user_tz": -180
    },
    "id": "ej0sUvnZzdag",
    "outputId": "2939095f-9f76-45f4-8b1a-9a314b70026a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_summary(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                         Top Layers Removed:\n",
    "# flatten (Flatten)            (None, 25088)             0         \n",
    "# _________________________________________________________________\n",
    "# fc1 (Dense)                  (None, 4096)              102764544 \n",
    "# _________________________________________________________________\n",
    "# fc2 (Dense)                  (None, 4096)              16781312  \n",
    "# _________________________________________________________________\n",
    "# predictions (Dense)          (None, 1000)              4097000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facenet = load_model('facenet')\n",
    "print_summary(facenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VggFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vggface = load_model('vgg_face')\n",
    "print_summary(vggface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYSE1ux4OYW8"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx6hmWN3OYW-"
   },
   "source": [
    "Adding new model `model` whose first layer is `vggface/vgg_face` with additional layers \n",
    "(from `tensorflow.keras.layers`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_toplayers(base_model):\n",
    "    \"\"\"\n",
    "    Function takes basemodel and add top layers\n",
    "    \"\"\"\n",
    "    base_model.trainable=False\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print_summary(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = 2\n",
    "# base_model_output = Sequential()\n",
    "# base_model_output = Convolution2D(classes, (1, 1), name='predictions')(basemodel.layers[-4].output)\n",
    "# base_model_output = Flatten()(base_model_output)\n",
    "# base_model_output = Dense(128, activation='relu')(base_model_output)\n",
    "# base_model_output = Dense(2, activation='relu')(base_model_output)\n",
    "# base_model_output = Activation('relu')(base_model_output)\n",
    "# model = Model(inputs=vggface.input, outputs=base_model_output)\n",
    "# -----------------------------------------------------------\n",
    "# model = Sequential()\n",
    "# model.add(basemodel)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# ------------------------------------------------------------\n",
    "# model = Sequential()\n",
    "# model = Convolution2D(classes, (1, 1), name='predictions')(basemodel.layers[-4].output)\n",
    "# model = Flatten()(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = Flatten()(model)\n",
    "# model = Convolution2D(64, 3, padding='same', input_shape=(32,32,3))(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = Convolution2D(64, (3, 3))(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = MaxPooling2D(pool_size=(2, 2))(model)\n",
    "# model = Dropout(0.25)(model)\n",
    "# model = Convolution2D(32, (3, 3), padding='same')(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = Convolution2D(32, (3, 3))(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = MaxPooling2D(pool_size=(2, 2))(model)\n",
    "# model = Dropout(0.25)(model)\n",
    "# model = Flatten()(model)\n",
    "# model = Dense(512)(model)\n",
    "# model = Activation('relu')(model)\n",
    "# model = Dropout(0.5)(model)\n",
    "# model = Dense(10, activation='relu')(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djCoifqrOYW_"
   },
   "source": [
    "To train our transfer learning model we will freeze the weights of the basemodel and only train the added layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_vgg16 = adding_toplayers(vgg16)\n",
    "mod_facenet = adding_toplayers(facenet)\n",
    "mod_vggface = adding_toplayers(vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mod_vgg16, mod_vggface, mod_facenet]\n",
    "model = mod_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [earlystopper, checkpoint]\n",
    "\n",
    "# Optimzing\n",
    "model.compile(optimizers.RMSprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# Fitting\n",
    "STEP_SIZE_TRAIN=train_data.n//train_data.batch_size\n",
    "STEP_SIZE_VALID=valid_data.n//valid_data.batch_size\n",
    "STEP_SIZE_TEST=test_data.n//test_data.batch_size\n",
    "EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data,\n",
    "          steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "          validation_data=valid_data,\n",
    "          validation_steps=STEP_SIZE_VALID,\n",
    "          callbacks=callback_list,\n",
    "          epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(valid_data,steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss:\\t{round(loss,2)}\\nAcc.:\\t{round(acc,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(18,5))\n",
    "# ax.plot(np.arange(1, EPOCH), history.history[\"loss\"], label=\"train\")\n",
    "# plt.plot(np.arange(1, EPOCH), history.history[\"val_loss\"], label=\"val\")\n",
    "# plt.plot(np.arange(1, EPOCH), history.history[\"accuracy\"], label=\"train\")\n",
    "# plt.plot(np.arange(1, EPOCH), history.history[\"val_accuracy\"], label=\"val\")\n",
    "# plt.suptitle(\"Val & Train Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Acc\")\n",
    "# plt.legend(loc=\"middle right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Compute the labels from the normalized confusion matrix.\n",
    "    labels = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames_test = test_data.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6-V2ggbnWlj"
   },
   "outputs": [],
   "source": [
    "test_data.reset()\n",
    "pred = model.predict(test_data,\n",
    "                     steps=STEP_SIZE_TEST,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = list(map(lambda x: 1 if float(x) >= 0.2 else 0 , pred))# {1.0:'With',0.0:'W/O'}\n",
    "pd.DataFrame(data=(filenames_test, y_pred, test['files'], test['label'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices = np.argmax(pred, axis=1)\n",
    "labels = (train_data.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=test_data.filenames\n",
    "s = len(predictions)\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results['Predictions'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(img_path, actions = [], models = {}, enforce_detection = True, detector_backend = 'opencv'):\n",
    "\n",
    "    if type(img_path) == list:\n",
    "        img_paths = img_path.copy()\n",
    "        bulkProcess = True\n",
    "    else:\n",
    "        img_paths = [img_path]\n",
    "        bulkProcess = False\n",
    "\n",
    "    #---------------------------------\n",
    "\n",
    "    #if a specific target is not passed, then find them all\n",
    "    if len(actions) == 0:\n",
    "        actions= ['emotion', 'age', 'gender', 'race', 'eyeglass']\n",
    "\n",
    "    #print(\"Actions to do: \", actions)\n",
    "\n",
    "    #---------------------------------\n",
    "\n",
    "    if 'emotion' in actions:\n",
    "        if 'emotion' in models:\n",
    "            print(\"already built emotion model is passed\")\n",
    "            emotion_model = models['emotion']\n",
    "        else:\n",
    "            emotion_model = Emotion.loadModel()\n",
    "\n",
    "    if 'age' in actions:\n",
    "        if 'age' in models:\n",
    "            print(\"already built age model is passed\")\n",
    "            age_model = models['age']\n",
    "        else:\n",
    "            age_model = Age.loadModel()\n",
    "\n",
    "    if 'gender' in actions:\n",
    "        if 'gender' in models:\n",
    "            print(\"already built gender model is passed\")\n",
    "            gender_model = models['gender']\n",
    "        else:\n",
    "            gender_model = Gender.loadModel()\n",
    "\n",
    "    if 'race' in actions:\n",
    "        if 'race' in models:\n",
    "            print(\"already built race model is passed\")\n",
    "            race_model = models['race']\n",
    "        else:\n",
    "            race_model = Race.loadModel()\n",
    "            \n",
    "    if 'eyeglass' in actions:\n",
    "        if 'eyeglass' in models:\n",
    "            print(\"already built race model is passed\")\n",
    "            eyeglass_model = models['eyeglass']\n",
    "        else:\n",
    "            eyeglass_model = model.load_weightsweights('eyeglass.h5')\n",
    "    #---------------------------------\n",
    "\n",
    "    resp_objects = []\n",
    "\n",
    "    disable_option = False if len(img_paths) > 1 else True\n",
    "\n",
    "    global_pbar = tqdm(range(0,len(img_paths)), desc='Analyzing', disable = disable_option)\n",
    "\n",
    "    #for img_path in img_paths:\n",
    "    for j in global_pbar:\n",
    "        img_path = img_paths[j]\n",
    "\n",
    "        resp_obj = \"{\"\n",
    "\n",
    "        disable_option = False if len(actions) > 1 else True\n",
    "\n",
    "        pbar = tqdm(range(0,len(actions)), desc='Finding actions', disable = disable_option)\n",
    "\n",
    "        action_idx = 0\n",
    "        img_224 = None # Set to prevent re-detection\n",
    "        #for action in actions:\n",
    "        for index in pbar:\n",
    "            action = actions[index]\n",
    "            pbar.set_description(\"Action: %s\" % (action))\n",
    "\n",
    "            if action_idx > 0:\n",
    "                resp_obj += \", \"\n",
    "\n",
    "            if action == 'emotion':\n",
    "                emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "                img = functions.preprocess_face(img = img_path, target_size = (48, 48), grayscale = True, enforce_detection = enforce_detection, detector_backend = detector_backend)\n",
    "\n",
    "                emotion_predictions = emotion_model.predict(img)[0,:]\n",
    "\n",
    "                sum_of_predictions = emotion_predictions.sum()\n",
    "\n",
    "                emotion_obj = \"\\\"emotion\\\": {\"\n",
    "                for i in range(0, len(emotion_labels)):\n",
    "                    emotion_label = emotion_labels[i]\n",
    "                    emotion_prediction = 100 * emotion_predictions[i] / sum_of_predictions\n",
    "\n",
    "                    if i > 0: emotion_obj += \", \"\n",
    "\n",
    "                    emotion_obj += \"\\\"%s\\\": %s\" % (emotion_label, emotion_prediction)\n",
    "\n",
    "                emotion_obj += \"}\"\n",
    "\n",
    "                emotion_obj += \", \\\"dominant_emotion\\\": \\\"%s\\\"\" % (emotion_labels[np.argmax(emotion_predictions)])\n",
    "\n",
    "                resp_obj += emotion_obj\n",
    "\n",
    "            elif action == 'age':\n",
    "                if img_224 is None:\n",
    "                    img_224 = functions.preprocess_face(img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection) #just emotion model expects grayscale images\n",
    "                #print(\"age prediction\")\n",
    "                age_predictions = age_model.predict(img_224)[0,:]\n",
    "                apparent_age = Age.findApparentAge(age_predictions)\n",
    "\n",
    "                resp_obj += \"\\\"age\\\": %s\" % (apparent_age)\n",
    "\n",
    "            elif action == 'gender':\n",
    "                if img_224 is None:\n",
    "                    img_224 = functions.preprocess_face(img = img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection, detector_backend = detector_backend) #just emotion model expects grayscale images\n",
    "                #print(\"gender prediction\")\n",
    "\n",
    "                gender_prediction = gender_model.predict(img_224)[0,:]\n",
    "\n",
    "                if np.argmax(gender_prediction) == 0:\n",
    "                    gender = \"Woman\"\n",
    "                elif np.argmax(gender_prediction) == 1:\n",
    "                    gender = \"Man\"\n",
    "            \n",
    "            elif action == 'eyeglass':\n",
    "                if img_224 is None:\n",
    "                    img_224 = functions.preprocess_face(img = img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection, detector_backend = detector_backend) #just emotion model expects grayscale images\n",
    "                #print(\"gender prediction\")\n",
    "\n",
    "                eyeglass_prediction = eyeglass_model.predict(img_224)[0,:]\n",
    "\n",
    "                if np.argmax(eyeglass_prediction) == 0:\n",
    "                    eg = \"W/O Eyeglasses\"\n",
    "                elif np.argmax(eyeglass_prediction) == 1:\n",
    "                    eg = \"With Eyeglasses\"\n",
    "\n",
    "\n",
    "                resp_obj += \"\\\"eyeglass\\\": \\\"%s\\\"\" % (eg)\n",
    "\n",
    "            elif action == 'race':\n",
    "                if img_224 is None:\n",
    "                    img_224 = functions.preprocess_face(img = img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection, detector_backend = detector_backend) #just emotion model expects grayscale images\n",
    "                race_predictions = race_model.predict(img_224)[0,:]\n",
    "                race_labels = ['asian', 'indian', 'black', 'white', 'middle eastern', 'latino hispanic']\n",
    "\n",
    "                sum_of_predictions = race_predictions.sum()\n",
    "\n",
    "                race_obj = \"\\\"race\\\": {\"\n",
    "                for i in range(0, len(race_labels)):\n",
    "                    race_label = race_labels[i]\n",
    "                    race_prediction = 100 * race_predictions[i] / sum_of_predictions\n",
    "\n",
    "                    if i > 0: race_obj += \", \"\n",
    "\n",
    "                    race_obj += \"\\\"%s\\\": %s\" % (race_label, race_prediction)\n",
    "\n",
    "                race_obj += \"}\"\n",
    "                race_obj += \", \\\"dominant_race\\\": \\\"%s\\\"\" % (race_labels[np.argmax(race_predictions)])\n",
    "\n",
    "                resp_obj += race_obj\n",
    "\n",
    "            action_idx = action_idx + 1\n",
    "\n",
    "        resp_obj += \"}\"\n",
    "\n",
    "        resp_obj = json.loads(resp_obj)\n",
    "\n",
    "        if bulkProcess == True:\n",
    "            resp_objects.append(resp_obj)\n",
    "        else:\n",
    "            return resp_obj\n",
    "\n",
    "    if bulkProcess == True:\n",
    "        resp_obj = \"{\"\n",
    "\n",
    "        for i in range(0, len(resp_objects)):\n",
    "            resp_item = json.dumps(resp_objects[i])\n",
    "\n",
    "            if i > 0:\n",
    "                resp_obj += \", \"\n",
    "\n",
    "            resp_obj += \"\\\"instance_\"+str(i+1)+\"\\\": \"+resp_item\n",
    "        resp_obj += \"}\"\n",
    "        resp_obj = json.loads(resp_obj)\n",
    "        return resp_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_face(df, backend=0):\n",
    "    \"\"\"\n",
    "    Function call image file as str or from dataframe and analyze it with deepface module to extract\n",
    "    race, age, gender and emotion\n",
    "    \"\"\"\n",
    "    \n",
    "    # reading file\n",
    "    if isinstance(df, str):\n",
    "        file = find_imagepath(df)\n",
    "    else:\n",
    "        img_f1 = df.sample().values[0]\n",
    "        file = find_imagepath(img_f1)\n",
    "    \n",
    "    # Run DeepFace\n",
    "    try:\n",
    "        backends = ['opencv', 'ssd', 'dlib', 'mtcnn']\n",
    "        demography = DeepFace.analyze(file, detector_backend = backends[backend])\n",
    "        age = int(demography['age'])\n",
    "        gender = demography['gender']\n",
    "        emotion = demography['dominant_emotion']\n",
    "        race = demography['dominant_race']\n",
    "        textstr = f'Age:\\t\\t{age}\\nGender:\\t\\t{gender}\\nRace:\\t\\t{race.title()}\\nEmotion:\\t{emotion.title()}'\n",
    "\n",
    "    except ValueError: \n",
    "        print('Face could not be detected')\n",
    "        sys.exit()\n",
    "        \n",
    "    # Plot\n",
    "    plt.figure(figsize=(5,5))\n",
    "    img = mpimg.imread(file)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    print(textstr)\n",
    "# DeepFace.verify(\"img1.jpg\", \"img2.jpg\", model_name = \"VGG-Face\", model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eyeglass_model = tf.saved_model.load(os.path.join(os.getcwd(), 'eyeglass.h5'))\n",
    "model.load('eyeglass.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_shapes = ([90000], ())\n",
    "analyze_face(results['Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFace.stream(IMAGEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CV_Ex1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
